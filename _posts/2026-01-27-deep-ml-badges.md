---
layout: post
title: Earned Deep-ML Badges - Attention Is All You Need & ResNet
date: 2026-01-27 22:44:00 +0900
description: Celebrating the completion of Deep-ML problem collections on Transformers and ResNet architectures.
tags: deep-learning certifications machine-learning
categories: achievements
giscus_comments: false
related_posts: false
---

I'm excited to share that I've earned two badges from [Deep-ML](https://www.deep-ml.com), a platform focused on deep learning problem-solving!

## üèÜ Certifications Earned

### Attention Is All You Need Badge
Successfully completed all problems in the **Attention Is All You Need** collection, covering the foundational concepts of the Transformer architecture that revolutionized NLP and beyond.

- **Issued:** January 27, 2026
- **Credential ID:** `DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5`
- **Verify:** [View Credential](https://www.deep-ml.com/credentials/DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5)

---

### ResNet Badge
Successfully completed all problems in the **ResNet** collection, mastering residual learning and skip connections that enabled training of very deep neural networks.

- **Issued:** January 27, 2026
- **Credential ID:** `DMLBADGE-RESNET-mkvzwgo0`
- **Verify:** [View Credential](https://www.deep-ml.com/credentials/DMLBADGE-RESNET-mkvzwgo0)

---

## Why These Matter

Both **Transformers** and **ResNet** are foundational architectures in modern deep learning:

- **Transformers** power state-of-the-art models in NLP (BERT, GPT), computer vision (ViT), and even speech recognition (like the ASR models I work on at Samsung).
- **ResNet** introduced residual connections that solved the vanishing gradient problem, enabling networks with hundreds of layers to be trained effectively.

These hands-on problem collections helped reinforce my understanding of the underlying mechanics‚Äîfrom self-attention mechanisms to skip connections.

## What's Next?

I'm looking forward to tackling more Deep-ML collections and continuing to deepen my practical understanding of deep learning architectures. Stay tuned for more updates!
