<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yashjayswal24.github.io/portfolio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yashjayswal24.github.io/portfolio/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-27T15:06:15+00:00</updated><id>https://yashjayswal24.github.io/portfolio/feed.xml</id><title type="html">Yash Jayswal</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Building Autograd: Chain Rule and Topo-Sort</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/building-autograd/" rel="alternate" type="text/html" title="Building Autograd: Chain Rule and Topo-Sort"/><published>2026-01-27T15:05:00+00:00</published><updated>2026-01-27T15:05:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/building-autograd</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/building-autograd/"><![CDATA[<p>I recently tackled the ‚ÄúBasic Autograd Operations‚Äù problem on <a href="https://www.deep-ml.com/">Deep-ML</a>, and it was an eye-opening experience. Following the footsteps of Andrej Karpathy (check out his <a href="https://youtu.be/VMj-3S1tku0?si=gjlnFP4o3JRN9dTg">legendary Micrograd video</a>), I built a scalar-valued autograd engine.</p> <h2 id="-bridging-the-gap-theory-to-code">üåâ Bridging the Gap: Theory to Code</h2> <p>What I loved most about this project was how systematic the commands felt. The chain rule isn‚Äôt just a math formula here; it‚Äôs a series of small, local instructions that each <code class="language-plaintext highlighter-rouge">Value</code> object knows how to execute.</p> <h3 id="-topological-sorting-in-the-wild">üîÑ Topological Sorting in the Wild</h3> <p>For a long time, I only saw <strong>Topological Sorting</strong> in competitive programming contexts (LeetCode/CodeForces). But in an autograd engine, it‚Äôs the ‚Äúsecret sauce.‚Äù It ensures that before we compute the gradient for any node, we‚Äôve already finished with everything that depends on it.</p> <p>This simple algorithm turns a messy graph of operations into a perfectly ordered sequence for backpropagation.</p> <h3 id="-the-elegance-of-python">üêç The Elegance of Python</h3> <p>The simplicity with which we can write this in Python is stunning. One single <code class="language-plaintext highlighter-rouge">backward()</code> function can set all gradients across the entire network, mirroring the core functionality of heavy-weights like PyTorch.</p> <hr/> <h3 id="-the-implementation">üíª The Implementation</h3> <p>Here is my <code class="language-plaintext highlighter-rouge">Value</code> class that handles the heavy lifting:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">_children</span><span class="o">=</span><span class="p">(),</span> <span class="n">_op</span><span class="o">=</span><span class="sh">''</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">_children</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">_op</span>
    
    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="sh">'</span><span class="s">+</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="sh">'</span><span class="s">*</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,),</span> <span class="sh">'</span><span class="s">ReLU</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                <span class="n">visited</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="p">.</span><span class="n">_prev</span><span class="p">:</span>
                    <span class="nf">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
                <span class="n">topo</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="nf">build_topo</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
            <span class="n">node</span><span class="p">.</span><span class="nf">_backward</span><span class="p">()</span>
</code></pre></div></div> <p>Check out the full repository and tests here: üëâ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/autograd-operations">Autograd Project on GitHub</a></strong></p> <hr/> <p><em>Next up: Building actual neurons using this autograd engine!</em></p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="autograd"/><category term="micrograd"/><category term="python"/><summary type="html"><![CDATA[Implementing a basic autograd engine from scratch and discovering the real-world power of topological sorting.]]></summary></entry><entry><title type="html">Understanding LSTMs: A Hands-on Intuition</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms/" rel="alternate" type="text/html" title="Understanding LSTMs: A Hands-on Intuition"/><published>2026-01-27T14:55:00+00:00</published><updated>2026-01-27T14:55:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms/"><![CDATA[<p>I recently implemented a <strong>Long Short-Term Memory (LSTM)</strong> network from scratch using NumPy. While the formulas can look daunting, the real beauty of LSTMs lies in their intuitive design as a ‚Äúcontrolled memory stream.‚Äù</p> <h2 id="-the-controlled-memory-intuition">üß† The ‚ÄúControlled Memory‚Äù Intuition</h2> <p>If you‚Äôve ever struggled with the technicalities of LSTMs, I highly recommend reading <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Christopher Olah‚Äôs classic blog post</a>. It‚Äôs the gold standard for understanding how these networks truly function.</p> <p>In my implementation, I focused on making the gates ‚Äúfeel‚Äù like what they actually do:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">*</code> (Multiplication) is the Selector:</strong> It decides exactly how much information flow should be allowed at any given moment.</li> <li><strong><code class="language-plaintext highlighter-rouge">+</code> (Addition) is the Knowledge Adder:</strong> It creates a ‚Äúhighway‚Äù for the cell state, allowing gradients to flow without vanishing‚Äîsolving the key problem of traditional RNNs.</li> <li><strong><code class="language-plaintext highlighter-rouge">Sigmoid</code> is the Gatekeeper:</strong> Because it outputs between 0 and 1, it‚Äôs perfect for selecting or blocking information.</li> <li><strong><code class="language-plaintext highlighter-rouge">Tanh</code> is the Featurizer:</strong> It‚Äôs great for creating new candidate features because it normalizes data while keeping it expressive.</li> </ul> <h2 id="-what-each-gate-feels-like">üö™ What each gate ‚Äúfeels‚Äù like</h2> <ol> <li><strong>Forget Gate:</strong> <em>The Eraser.</em> It looks at the past and asks: ‚ÄúIs this still worth remembering?‚Äù</li> <li><strong>Input Gate:</strong> <em>The Filter.</em> It decides which parts of the <em>new</em> input are actually useful.</li> <li><strong>Candidate Memory:</strong> <em>The Writer.</em> It prepares the new ‚Äúdraft‚Äù of information using <code class="language-plaintext highlighter-rouge">tanh</code>.</li> <li><strong>Output Gate:</strong> <em>The Presenter.</em> It takes the long-term memory and decides what to show for the current time step.</li> </ol> <hr/> <h3 id="-implementation">üíª Implementation</h3> <p>You can find my full implementation, along with a detailed breakdown of the intuition, in my dedicated repository:</p> <p>üëâ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/lstm-network">LSTM Implementation on GitHub</a></strong></p> <p>This project is part of my ongoing journey through the <a href="https://www.deep-ml.com/">Deep-ML</a> problem sets!</p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="machine-learning"/><category term="lstm"/><summary type="html"><![CDATA[Implementing Long Short-Term Memory (LSTM) from scratch and exploring the intuition behind its gates.]]></summary></entry><entry><title type="html">Earned Deep-ML Badges - Attention Is All You Need &amp;amp; ResNet</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges/" rel="alternate" type="text/html" title="Earned Deep-ML Badges - Attention Is All You Need &amp;amp; ResNet"/><published>2026-01-27T13:44:00+00:00</published><updated>2026-01-27T13:44:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges/"><![CDATA[<p>I‚Äôm excited to share that I‚Äôve earned two badges from <a href="https://www.deep-ml.com">Deep-ML</a>, a platform focused on deep learning problem-solving!</p> <h2 id="-certifications-earned">üèÜ Certifications Earned</h2> <h3 id="attention-is-all-you-need-badge">Attention Is All You Need Badge</h3> <p>Successfully completed all problems in the <strong>Attention Is All You Need</strong> collection, covering the foundational concepts of the Transformer architecture that revolutionized NLP and beyond.</p> <ul> <li><strong>Issued:</strong> January 27, 2026</li> <li><strong>Credential ID:</strong> <code class="language-plaintext highlighter-rouge">DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5</code></li> <li><strong>Verify:</strong> <a href="https://www.deep-ml.com/credentials/DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5">View Credential</a></li> </ul> <hr/> <h3 id="resnet-badge">ResNet Badge</h3> <p>Successfully completed all problems in the <strong>ResNet</strong> collection, mastering residual learning and skip connections that enabled training of very deep neural networks.</p> <ul> <li><strong>Issued:</strong> January 27, 2026</li> <li><strong>Credential ID:</strong> <code class="language-plaintext highlighter-rouge">DMLBADGE-RESNET-mkvzwgo0</code></li> <li><strong>Verify:</strong> <a href="https://www.deep-ml.com/credentials/DMLBADGE-RESNET-mkvzwgo0">View Credential</a></li> </ul> <hr/> <h2 id="why-these-matter">Why These Matter</h2> <p>Both <strong>Transformers</strong> and <strong>ResNet</strong> are foundational architectures in modern deep learning:</p> <ul> <li><strong>Transformers</strong> power state-of-the-art models in NLP (BERT, GPT), computer vision (ViT), and even speech recognition (like the ASR models I work on at Samsung).</li> <li><strong>ResNet</strong> introduced residual connections that solved the vanishing gradient problem, enabling networks with hundreds of layers to be trained effectively.</li> </ul> <p>These hands-on problem collections helped reinforce my understanding of the underlying mechanics‚Äîfrom self-attention mechanisms to skip connections.</p> <h2 id="whats-next">What‚Äôs Next?</h2> <p>I‚Äôm looking forward to tackling more Deep-ML collections and continuing to deepen my practical understanding of deep learning architectures. Stay tuned for more updates!</p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="certifications"/><category term="machine-learning"/><summary type="html"><![CDATA[Celebrating the completion of Deep-ML problem collections on Transformers and ResNet architectures.]]></summary></entry><entry><title type="html">Hello World</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/hello-world/" rel="alternate" type="text/html" title="Hello World"/><published>2026-01-23T21:51:00+00:00</published><updated>2026-01-23T21:51:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/hello-world</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/hello-world/"><![CDATA[<p>Welcome to my new portfolio website!</p> <p>This is the first post on my new blog. I‚Äôll be sharing updates about my projects, research, and other interests here.</p> <p>Stay tuned for more!</p>]]></content><author><name></name></author><category term="general"/><category term="introduction"/><summary type="html"><![CDATA[Welcome to my new portfolio website!]]></summary></entry></feed>