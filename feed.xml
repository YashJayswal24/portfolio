<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yashjayswal24.github.io/portfolio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yashjayswal24.github.io/portfolio/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-27T14:59:10+00:00</updated><id>https://yashjayswal24.github.io/portfolio/feed.xml</id><title type="html">Yash Jayswal</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding LSTMs: A Hands-on Intuition</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms/" rel="alternate" type="text/html" title="Understanding LSTMs: A Hands-on Intuition"/><published>2026-01-27T14:55:00+00:00</published><updated>2026-01-27T14:55:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms/"><![CDATA[<p>I recently implemented a <strong>Long Short-Term Memory (LSTM)</strong> network from scratch using NumPy. While the formulas can look daunting, the real beauty of LSTMs lies in their intuitive design as a â€œcontrolled memory stream.â€</p> <h2 id="-the-controlled-memory-intuition">ğŸ§  The â€œControlled Memoryâ€ Intuition</h2> <p>If youâ€™ve ever struggled with the technicalities of LSTMs, I highly recommend reading <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Christopher Olahâ€™s classic blog post</a>. Itâ€™s the gold standard for understanding how these networks truly function.</p> <p>In my implementation, I focused on making the gates â€œfeelâ€ like what they actually do:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">*</code> (Multiplication) is the Selector:</strong> It decides exactly how much information flow should be allowed at any given moment.</li> <li><strong><code class="language-plaintext highlighter-rouge">+</code> (Addition) is the Knowledge Adder:</strong> It creates a â€œhighwayâ€ for the cell state, allowing gradients to flow without vanishingâ€”solving the key problem of traditional RNNs.</li> <li><strong><code class="language-plaintext highlighter-rouge">Sigmoid</code> is the Gatekeeper:</strong> Because it outputs between 0 and 1, itâ€™s perfect for selecting or blocking information.</li> <li><strong><code class="language-plaintext highlighter-rouge">Tanh</code> is the Featurizer:</strong> Itâ€™s great for creating new candidate features because it normalizes data while keeping it expressive.</li> </ul> <h2 id="-what-each-gate-feels-like">ğŸšª What each gate â€œfeelsâ€ like</h2> <ol> <li><strong>Forget Gate:</strong> <em>The Eraser.</em> It looks at the past and asks: â€œIs this still worth remembering?â€</li> <li><strong>Input Gate:</strong> <em>The Filter.</em> It decides which parts of the <em>new</em> input are actually useful.</li> <li><strong>Candidate Memory:</strong> <em>The Writer.</em> It prepares the new â€œdraftâ€ of information using <code class="language-plaintext highlighter-rouge">tanh</code>.</li> <li><strong>Output Gate:</strong> <em>The Presenter.</em> It takes the long-term memory and decides what to show for the current time step.</li> </ol> <hr/> <h3 id="-implementation">ğŸ’» Implementation</h3> <p>You can find my full implementation, along with a detailed breakdown of the intuition, in my dedicated repository:</p> <p>ğŸ‘‰ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/lstm-network">LSTM Implementation on GitHub</a></strong></p> <p>This project is part of my ongoing journey through the <a href="https://www.deep-ml.com/">Deep-ML</a> problem sets!</p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="machine-learning"/><category term="lstm"/><summary type="html"><![CDATA[Implementing Long Short-Term Memory (LSTM) from scratch and exploring the intuition behind its gates.]]></summary></entry><entry><title type="html">Earned Deep-ML Badges - Attention Is All You Need &amp;amp; ResNet</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges/" rel="alternate" type="text/html" title="Earned Deep-ML Badges - Attention Is All You Need &amp;amp; ResNet"/><published>2026-01-27T13:44:00+00:00</published><updated>2026-01-27T13:44:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges/"><![CDATA[<p>Iâ€™m excited to share that Iâ€™ve earned two badges from <a href="https://www.deep-ml.com">Deep-ML</a>, a platform focused on deep learning problem-solving!</p> <h2 id="-certifications-earned">ğŸ† Certifications Earned</h2> <h3 id="attention-is-all-you-need-badge">Attention Is All You Need Badge</h3> <p>Successfully completed all problems in the <strong>Attention Is All You Need</strong> collection, covering the foundational concepts of the Transformer architecture that revolutionized NLP and beyond.</p> <ul> <li><strong>Issued:</strong> January 27, 2026</li> <li><strong>Credential ID:</strong> <code class="language-plaintext highlighter-rouge">DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5</code></li> <li><strong>Verify:</strong> <a href="https://www.deep-ml.com/credentials/DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5">View Credential</a></li> </ul> <hr/> <h3 id="resnet-badge">ResNet Badge</h3> <p>Successfully completed all problems in the <strong>ResNet</strong> collection, mastering residual learning and skip connections that enabled training of very deep neural networks.</p> <ul> <li><strong>Issued:</strong> January 27, 2026</li> <li><strong>Credential ID:</strong> <code class="language-plaintext highlighter-rouge">DMLBADGE-RESNET-mkvzwgo0</code></li> <li><strong>Verify:</strong> <a href="https://www.deep-ml.com/credentials/DMLBADGE-RESNET-mkvzwgo0">View Credential</a></li> </ul> <hr/> <h2 id="why-these-matter">Why These Matter</h2> <p>Both <strong>Transformers</strong> and <strong>ResNet</strong> are foundational architectures in modern deep learning:</p> <ul> <li><strong>Transformers</strong> power state-of-the-art models in NLP (BERT, GPT), computer vision (ViT), and even speech recognition (like the ASR models I work on at Samsung).</li> <li><strong>ResNet</strong> introduced residual connections that solved the vanishing gradient problem, enabling networks with hundreds of layers to be trained effectively.</li> </ul> <p>These hands-on problem collections helped reinforce my understanding of the underlying mechanicsâ€”from self-attention mechanisms to skip connections.</p> <h2 id="whats-next">Whatâ€™s Next?</h2> <p>Iâ€™m looking forward to tackling more Deep-ML collections and continuing to deepen my practical understanding of deep learning architectures. Stay tuned for more updates!</p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="certifications"/><category term="machine-learning"/><summary type="html"><![CDATA[Celebrating the completion of Deep-ML problem collections on Transformers and ResNet architectures.]]></summary></entry><entry><title type="html">Hello World</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/hello-world/" rel="alternate" type="text/html" title="Hello World"/><published>2026-01-23T21:51:00+00:00</published><updated>2026-01-23T21:51:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/hello-world</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/hello-world/"><![CDATA[<p>Welcome to my new portfolio website!</p> <p>This is the first post on my new blog. Iâ€™ll be sharing updates about my projects, research, and other interests here.</p> <p>Stay tuned for more!</p>]]></content><author><name></name></author><category term="general"/><category term="introduction"/><summary type="html"><![CDATA[Welcome to my new portfolio website!]]></summary></entry></feed>