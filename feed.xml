<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yashjayswal24.github.io/portfolio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yashjayswal24.github.io/portfolio/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-28T12:32:25+00:00</updated><id>https://yashjayswal24.github.io/portfolio/feed.xml</id><title type="html">Yash Jayswal</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">3 New Deep-ML Badges Earned! ğŸ†</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges-new/" rel="alternate" type="text/html" title="3 New Deep-ML Badges Earned! ğŸ†"/><published>2026-01-28T12:30:00+00:00</published><updated>2026-01-28T12:30:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges-new</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges-new/"><![CDATA[<p>Iâ€™m thrilled to announce that Iâ€™ve earned <strong>3 new badges</strong> on <a href="https://www.deep-ml.com/">Deep-ML</a> today!</p> <h2 id="ï¸-badges-earned">ğŸ–ï¸ Badges Earned</h2> <table> <thead> <tr> <th>Badge</th> <th>Collection</th> <th>Credential ID</th> </tr> </thead> <tbody> <tr> <td><strong>Machine Learning Badge</strong></td> <td><a href="https://www.deep-ml.com/credentials/DMLBADGE-MACHINELEARNING-mkxs68e8">Machine Learning</a></td> <td><code class="language-plaintext highlighter-rouge">DMLBADGE-MACHINELEARNING-mkxs68e8</code></td> </tr> <tr> <td><strong>Deep Learning Badge</strong></td> <td><a href="https://www.deep-ml.com/credentials/DMLBADGE-DEEPLEARNING-mkxned67">Deep Learning</a></td> <td><code class="language-plaintext highlighter-rouge">DMLBADGE-DEEPLEARNING-mkxned67</code></td> </tr> <tr> <td><strong>DenseNet Badge</strong></td> <td><a href="https://www.deep-ml.com/credentials/DMLBADGE-DENSENET-mkxuj45v">DenseNet</a></td> <td><code class="language-plaintext highlighter-rouge">DMLBADGE-DENSENET-mkxuj45v</code></td> </tr> </tbody> </table> <h2 id="-what-i-learned">ğŸ§  What I Learned</h2> <p>Completing these collections taught me a wide range of skills:</p> <ul> <li><strong>Python &amp; PyTorch</strong> â€” Core implementation skills</li> <li><strong>Deep Learning</strong> â€” Neural network architectures from scratch</li> <li><strong>Machine Learning</strong> â€” Classical algorithms like SVMs and kernel methods</li> <li><strong>LLMs</strong> â€” GPT-2 architecture and text generation</li> <li><strong>Vision Models</strong> â€” CNNs, ResNet, DenseNet architectures</li> <li><strong>Convolution Operations</strong> â€” 2D convolutions, padding, stride</li> </ul> <h2 id="-my-solutions">ğŸ“‚ My Solutions</h2> <p>All my solutions are documented and available on GitHub:</p> <p>ğŸ‘‰ <strong><a href="https://github.com/YashJayswal24/deepml-solns">deepml-solns Repository</a></strong></p> <hr/> <p><em>On to the next challenge!</em></p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="machine-learning"/><category term="certifications"/><category term="achievements"/><summary type="html"><![CDATA[Celebrating completion of the Machine Learning, Deep Learning, and DenseNet collections on Deep-ML.]]></summary></entry><entry><title type="html">DenseNet Block: Brute-Force Feature Reuse</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/densenet-block/" rel="alternate" type="text/html" title="DenseNet Block: Brute-Force Feature Reuse"/><published>2026-01-28T09:50:00+00:00</published><updated>2026-01-28T09:50:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/densenet-block</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/densenet-block/"><![CDATA[<p>I implemented a <strong>DenseNet Dense Block</strong> from scratch on <a href="https://www.deep-ml.com/">Deep-ML</a>. DenseNetâ€™s core idea is simple: concatenate <em>all</em> previous feature maps into each layer.</p> <h2 id="-quick-learnings">ğŸ’¡ Quick Learnings</h2> <ul> <li><strong>Type Matters:</strong> Using <code class="language-plaintext highlighter-rouge">0</code> instead of <code class="language-plaintext highlighter-rouge">0.0</code> in <code class="language-plaintext highlighter-rouge">np.maximum</code> caused type conversion bugs. Details matter!</li> <li><strong>DenseNet Philosophy:</strong> Throw all the compute you have at the problem. Concatenate everything.</li> </ul> <h2 id="ï¸-densenet-vs-resnet">âš–ï¸ DenseNet vs. ResNet</h2> <table> <thead> <tr> <th>Feature</th> <th>DenseNet</th> <th>ResNet</th> </tr> </thead> <tbody> <tr> <td>Connection</td> <td>Concatenation</td> <td>Addition</td> </tr> <tr> <td>Parameters</td> <td>Fewer</td> <td>More</td> </tr> <tr> <td>GPU Memory</td> <td><strong>High</strong></td> <td>Low</td> </tr> <tr> <td>Training Speed</td> <td><strong>Slow</strong></td> <td>Fast</td> </tr> <tr> <td>Best For</td> <td>Max accuracy</td> <td>Practical deployment</td> </tr> </tbody> </table> <p><strong>Verdict:</strong> Same goal (gradient flow), but ResNet is more practical.</p> <hr/> <h2 id="-the-implementation">ğŸ’» The Implementation</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">dense_net_block</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">growth_rate</span><span class="p">,</span> <span class="n">kernels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">C0</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">.</span><span class="n">shape</span>
    
    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">zero</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">n_b</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span><span class="p">,</span> <span class="n">n_c</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">k_h</span><span class="p">,</span> <span class="n">k_w</span><span class="p">,</span> <span class="n">c_in</span><span class="p">,</span> <span class="n">c_out</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="sh">'</span><span class="s">zero</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">pad_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">k_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">pad_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">k_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">X_padded</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad_h</span><span class="p">,</span> <span class="n">pad_h</span><span class="p">),</span> <span class="p">(</span><span class="n">pad_w</span><span class="p">,</span> <span class="n">pad_w</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">constant</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_b</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_w</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_b</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_h</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_w</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">c_out</span><span class="p">):</span>
                        <span class="n">h_strt</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="n">stride</span>
                        <span class="n">h_end</span> <span class="o">=</span> <span class="n">h_strt</span> <span class="o">+</span> <span class="n">k_h</span>
                        <span class="n">w_strt</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">stride</span>
                        <span class="n">w_end</span> <span class="o">=</span> <span class="n">w_strt</span> <span class="o">+</span> <span class="n">k_w</span>
                        <span class="n">out</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X_padded</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">h_strt</span><span class="p">:</span><span class="n">h_end</span><span class="p">,</span> <span class="n">w_strt</span><span class="p">:</span><span class="n">w_end</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">c</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="n">out</span> <span class="o">=</span> <span class="n">input_data</span>
    <span class="k">for</span> <span class="n">lay_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nf">relu</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nf">conv2d</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">kernels</span><span class="p">[</span><span class="n">lay_idx</span><span class="p">])</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">input_data</span><span class="p">,</span> <span class="n">out</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_data</span>
</code></pre></div></div> <p>ğŸ‘‰ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/densenet-block">Full Code on GitHub</a></strong></p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="cnn"/><category term="densenet"/><category term="resnet"/><category term="python"/><summary type="html"><![CDATA[Implementing a DenseNet dense block from scratch and comparing it to ResNet.]]></summary></entry><entry><title type="html">Pegasos Kernel SVM: The Hardest Math So Far</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/pegasos-svm/" rel="alternate" type="text/html" title="Pegasos Kernel SVM: The Hardest Math So Far"/><published>2026-01-28T08:45:00+00:00</published><updated>2026-01-28T08:45:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/pegasos-svm</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/pegasos-svm/"><![CDATA[<p>I just tackled the <strong>Pegasos Kernel SVM</strong> problem on <a href="https://www.deep-ml.com/">Deep-ML</a>, and I have to sayâ€”this was by far the <strong>hardest concept mathematically</strong> Iâ€™ve encountered so far.</p> <h2 id="-two-worlds-of-svm">ğŸ§® Two Worlds of SVM</h2> <p>There are two main approaches to training SVMs:</p> <ol> <li><strong>Primal (Pegasos):</strong> Uses sub-gradient descent on the hinge loss directly.</li> <li><strong>Dual (SMO):</strong> Uses Lagrange multipliers to solve a constrained optimization problem.</li> </ol> <p>Both are math-heavy and require a solid understanding of optimization theory.</p> <h2 id="-what-i-learned">ğŸ’¡ What I Learned</h2> <ul> <li><strong>Lagrangian Multipliers:</strong> The dual formulation of SVM relies on Lagrangians for constrained optimizationâ€”a concept I mostly saw in physics before!</li> <li><strong>The Kernel Trick:</strong> Allows us to compute dot products in higher-dimensional spaces without ever going there explicitly.</li> <li><strong>A Note on the Deep-ML Solution:</strong> The problem description uses <code class="language-plaintext highlighter-rouge">Î±_i â† Î±_i + Î·_t(y_i - Î»*Î±_i)</code>. I found it clearer to separate the decay term: <code class="language-plaintext highlighter-rouge">Î±_i â† (1 - 1/t) * Î±_i + Î·_t * y_i</code>, which achieves the same mathematical result.</li> </ul> <h3 id="-best-resource">ğŸ“š Best Resource</h3> <p>I found this MIT resource invaluable: <a href="https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf">An Idiotâ€™s Guide to SVMs</a>. Donâ€™t let the title fool youâ€”itâ€™s a comprehensive guide that requires serious effort!</p> <hr/> <h2 id="-the-implementation">ğŸ’» The Implementation</h2> <p>Hereâ€™s my full Pegasos Kernel SVM implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">pegasos_kernel_svm</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="n">lambda_val</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Train a kernel SVM using the deterministic Pegasos algorithm.
    </span><span class="sh">"""</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="k">def</span> <span class="nf">linear_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">kernel_fun</span> <span class="o">=</span> <span class="n">linear_kernel</span> <span class="k">if</span> <span class="n">kernel</span> <span class="o">==</span> <span class="sh">'</span><span class="s">linear</span><span class="sh">'</span> <span class="k">else</span> <span class="k">lambda</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">:</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">lambda_val</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">alphas</span> <span class="o">*</span> <span class="n">labels</span> <span class="o">*</span> <span class="nf">kernel_fun</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">data</span><span class="p">))</span> <span class="o">+</span> <span class="n">bias</span>
            <span class="k">if</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">pred</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">alphas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">alphas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">bias</span> <span class="o">+=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">alphas</span><span class="p">.</span><span class="nf">tolist</span><span class="p">(),</span> <span class="n">bias</span>
</code></pre></div></div> <h3 id="test-results">Test Results</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Linear Kernel
</span><span class="n">alphas</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">pegasos_kernel_svm</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> 
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> 
    <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="n">lambda_val</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(([</span><span class="nf">round</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">],</span> <span class="nf">round</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="c1"># Output: ([100.0, 0.0, -100.0, -100.0], -937.4755)
</span>
<span class="c1"># RBF Kernel
</span><span class="n">alphas</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="nf">pegasos_kernel_svm</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> 
    <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> 
    <span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">lambda_val</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="nf">print</span><span class="p">(([</span><span class="nf">round</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">],</span> <span class="nf">round</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="c1"># Output: ([100.0, 99.0, -100.0, -100.0], -115.0)
</span></code></pre></div></div> <hr/> <p>ğŸ‘‰ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/pegasos-kernel-svm">Full Code on GitHub</a></strong></p> <p><em>On to the next challenge!</em></p>]]></content><author><name></name></author><category term="achievements"/><category term="machine-learning"/><category term="svm"/><category term="kernel-methods"/><category term="python"/><summary type="html"><![CDATA[Implementing a deterministic Pegasos algorithm for kernel SVMs and diving deep into the mathematics of support vector machines.]]></summary></entry><entry><title type="html">Building a Primitive GPT-2: Layers &amp;amp; Dimensions</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/building-gpt2/" rel="alternate" type="text/html" title="Building a Primitive GPT-2: Layers &amp;amp; Dimensions"/><published>2026-01-28T06:45:00+00:00</published><updated>2026-01-28T06:45:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/building-gpt2</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/building-gpt2/"><![CDATA[<p>I recently took on the challenge of implementing a simplified <strong>GPT-2 Text Generation</strong> function on <a href="https://www.deep-ml.com/">Deep-ML</a>. This exercise was a fantastic way to bring together separate conceptsâ€”embeddings, layer normalization, and autoregressionâ€”into one cohesive system.</p> <h2 id="-the-puzzle-pieces">ğŸ§© The Puzzle Pieces</h2> <p>Building a generator like this forces you to confront the reality of the architecture:</p> <ol> <li><strong>Embeddings:</strong> Combining word identity with position identity (<code class="language-plaintext highlighter-rouge">wte + wpe</code>).</li> <li><strong>Layer Norm:</strong> Stabilizing the features before projection.</li> <li><strong>The Loop:</strong> Autoregressively predicting the next token and feeding it back.</li> </ol> <h2 id="-lightbulb-moments">ğŸ’¡ Lightbulb Moments</h2> <h3 id="dimensions-are-everything">Dimensions are Everything</h3> <p>I used to get confused by the endless reshaping in PyTorch/NumPy. But during this project, I realized that <strong>noting down the dimensions</strong> (like <code class="language-plaintext highlighter-rouge">(seq_len, d_model)</code>) makes everything fall into place. It turns abstract math into a shape-matching puzzle.</p> <h3 id="layer-norm-vs-batch-norm">Layer Norm vs. Batch Norm</h3> <p>I got stuck on which axis to normalize for a while. Then it clicked: <strong>Layer Norm gives every feature a fair shot.</strong> Unlike Batch Norm which looks across the batch, Layer Norm standardizes the features for a <em>single example</em>, ensuring that no single feature dominates the gradients.</p> <h3 id="the-cost-of-memory-less-generation">The Cost of â€œMemory-lessâ€ Generation</h3> <p>Writing the generation loop manually really highlights the inefficiency. To generate the 5th token, we re-process the first 4. This redundancy is exactly why <strong>KV Caching</strong> is a critical optimization in modern LLM serving!</p> <hr/> <h3 id="-the-code">ğŸ’» The Code</h3> <p>You can see my simplified implementation here:</p> <p>ğŸ‘‰ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/gpt2-text-generation">GPT-2 Implementation on GitHub</a></strong></p> <p>It uses a dummy encoder and random weights, but the logic is identical to the real thing!</p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="nlp"/><category term="transformers"/><category term="gpt-2"/><category term="python"/><summary type="html"><![CDATA[Merging deep learning concepts to build a simplified GPT-2 text generator and understanding the importance of dimensions.]]></summary></entry><entry><title type="html">Building Autograd: Chain Rule and Topo-Sort</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/building-autograd/" rel="alternate" type="text/html" title="Building Autograd: Chain Rule and Topo-Sort"/><published>2026-01-27T15:05:00+00:00</published><updated>2026-01-27T15:05:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/building-autograd</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/building-autograd/"><![CDATA[<p>I recently tackled the â€œBasic Autograd Operationsâ€ problem on <a href="https://www.deep-ml.com/">Deep-ML</a>, and it was an eye-opening experience. Following the footsteps of Andrej Karpathy (check out his <a href="https://youtu.be/VMj-3S1tku0?si=gjlnFP4o3JRN9dTg">legendary Micrograd video</a>), I built a scalar-valued autograd engine.</p> <h2 id="-bridging-the-gap-theory-to-code">ğŸŒ‰ Bridging the Gap: Theory to Code</h2> <p>What I loved most about this project was how systematic the commands felt. The chain rule isnâ€™t just a math formula here; itâ€™s a series of small, local instructions that each <code class="language-plaintext highlighter-rouge">Value</code> object knows how to execute.</p> <h3 id="-topological-sorting-in-the-wild">ğŸ”„ Topological Sorting in the Wild</h3> <p>For a long time, I only saw <strong>Topological Sorting</strong> in competitive programming contexts (LeetCode/CodeForces). But in an autograd engine, itâ€™s the â€œsecret sauce.â€ It ensures that before we compute the gradient for any node, weâ€™ve already finished with everything that depends on it.</p> <p>This simple algorithm turns a messy graph of operations into a perfectly ordered sequence for backpropagation.</p> <h3 id="-the-elegance-of-python">ğŸ The Elegance of Python</h3> <p>The simplicity with which we can write this in Python is stunning. One single <code class="language-plaintext highlighter-rouge">backward()</code> function can set all gradients across the entire network, mirroring the core functionality of heavy-weights like PyTorch.</p> <hr/> <h3 id="-the-implementation">ğŸ’» The Implementation</h3> <p>Here is my <code class="language-plaintext highlighter-rouge">Value</code> class that handles the heavy lifting:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Value</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">_children</span><span class="o">=</span><span class="p">(),</span> <span class="n">_op</span><span class="o">=</span><span class="sh">''</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">0</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_prev</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">_children</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">_op</span>
    
    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="sh">'</span><span class="s">+</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Value</span><span class="p">)</span> <span class="k">else</span> <span class="nc">Value</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">other</span><span class="p">),</span> <span class="sh">'</span><span class="s">*</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">other</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
            <span class="n">other</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nc">Value</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="n">self</span><span class="p">,),</span> <span class="sh">'</span><span class="s">ReLU</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">_backward</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="n">data</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">out</span><span class="p">.</span><span class="n">grad</span>
        <span class="n">out</span><span class="p">.</span><span class="n">_backward</span> <span class="o">=</span> <span class="n">_backward</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">topo</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">build_topo</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                <span class="n">visited</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">v</span><span class="p">.</span><span class="n">_prev</span><span class="p">:</span>
                    <span class="nf">build_topo</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
                <span class="n">topo</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="nf">build_topo</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">topo</span><span class="p">):</span>
            <span class="n">node</span><span class="p">.</span><span class="nf">_backward</span><span class="p">()</span>
</code></pre></div></div> <p>Check out the full repository and tests here: ğŸ‘‰ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/autograd-operations">Autograd Project on GitHub</a></strong></p> <hr/> <p><em>Next up: Building actual neurons using this autograd engine!</em></p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="autograd"/><category term="micrograd"/><category term="python"/><summary type="html"><![CDATA[Implementing a basic autograd engine from scratch and discovering the real-world power of topological sorting.]]></summary></entry><entry><title type="html">Understanding LSTMs: A Hands-on Intuition</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms/" rel="alternate" type="text/html" title="Understanding LSTMs: A Hands-on Intuition"/><published>2026-01-27T14:55:00+00:00</published><updated>2026-01-27T14:55:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/understanding-lstms/"><![CDATA[<p>I recently implemented a <strong>Long Short-Term Memory (LSTM)</strong> network from scratch using NumPy. While the formulas can look daunting, the real beauty of LSTMs lies in their intuitive design as a â€œcontrolled memory stream.â€</p> <h2 id="-the-controlled-memory-intuition">ğŸ§  The â€œControlled Memoryâ€ Intuition</h2> <p>If youâ€™ve ever struggled with the technicalities of LSTMs, I highly recommend reading <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Christopher Olahâ€™s classic blog post</a>. Itâ€™s the gold standard for understanding how these networks truly function.</p> <p>In my implementation, I focused on making the gates â€œfeelâ€ like what they actually do:</p> <ul> <li><strong><code class="language-plaintext highlighter-rouge">*</code> (Multiplication) is the Selector:</strong> It decides exactly how much information flow should be allowed at any given moment.</li> <li><strong><code class="language-plaintext highlighter-rouge">+</code> (Addition) is the Knowledge Adder:</strong> It creates a â€œhighwayâ€ for the cell state, allowing gradients to flow without vanishingâ€”solving the key problem of traditional RNNs.</li> <li><strong><code class="language-plaintext highlighter-rouge">Sigmoid</code> is the Gatekeeper:</strong> Because it outputs between 0 and 1, itâ€™s perfect for selecting or blocking information.</li> <li><strong><code class="language-plaintext highlighter-rouge">Tanh</code> is the Featurizer:</strong> Itâ€™s great for creating new candidate features because it normalizes data while keeping it expressive.</li> </ul> <h2 id="-what-each-gate-feels-like">ğŸšª What each gate â€œfeelsâ€ like</h2> <ol> <li><strong>Forget Gate:</strong> <em>The Eraser.</em> It looks at the past and asks: â€œIs this still worth remembering?â€</li> <li><strong>Input Gate:</strong> <em>The Filter.</em> It decides which parts of the <em>new</em> input are actually useful.</li> <li><strong>Candidate Memory:</strong> <em>The Writer.</em> It prepares the new â€œdraftâ€ of information using <code class="language-plaintext highlighter-rouge">tanh</code>.</li> <li><strong>Output Gate:</strong> <em>The Presenter.</em> It takes the long-term memory and decides what to show for the current time step.</li> </ol> <hr/> <h3 id="-implementation">ğŸ’» Implementation</h3> <p>You can find my full implementation, along with a detailed breakdown of the intuition, in my dedicated repository:</p> <p>ğŸ‘‰ <strong><a href="https://github.com/YashJayswal24/deepml-solns/tree/main/lstm-network">LSTM Implementation on GitHub</a></strong></p> <p>This project is part of my ongoing journey through the <a href="https://www.deep-ml.com/">Deep-ML</a> problem sets!</p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="machine-learning"/><category term="lstm"/><summary type="html"><![CDATA[Implementing Long Short-Term Memory (LSTM) from scratch and exploring the intuition behind its gates.]]></summary></entry><entry><title type="html">Earned Deep-ML Badges - Attention Is All You Need &amp;amp; ResNet</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges/" rel="alternate" type="text/html" title="Earned Deep-ML Badges - Attention Is All You Need &amp;amp; ResNet"/><published>2026-01-27T13:44:00+00:00</published><updated>2026-01-27T13:44:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/deep-ml-badges/"><![CDATA[<p>Iâ€™m excited to share that Iâ€™ve earned two badges from <a href="https://www.deep-ml.com">Deep-ML</a>, a platform focused on deep learning problem-solving!</p> <h2 id="-certifications-earned">ğŸ† Certifications Earned</h2> <h3 id="attention-is-all-you-need-badge">Attention Is All You Need Badge</h3> <p>Successfully completed all problems in the <strong>Attention Is All You Need</strong> collection, covering the foundational concepts of the Transformer architecture that revolutionized NLP and beyond.</p> <ul> <li><strong>Issued:</strong> January 27, 2026</li> <li><strong>Credential ID:</strong> <code class="language-plaintext highlighter-rouge">DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5</code></li> <li><strong>Verify:</strong> <a href="https://www.deep-ml.com/credentials/DMLBADGE-ATTENTIONISALLYOUNEED-mkvyjfa5">View Credential</a></li> </ul> <hr/> <h3 id="resnet-badge">ResNet Badge</h3> <p>Successfully completed all problems in the <strong>ResNet</strong> collection, mastering residual learning and skip connections that enabled training of very deep neural networks.</p> <ul> <li><strong>Issued:</strong> January 27, 2026</li> <li><strong>Credential ID:</strong> <code class="language-plaintext highlighter-rouge">DMLBADGE-RESNET-mkvzwgo0</code></li> <li><strong>Verify:</strong> <a href="https://www.deep-ml.com/credentials/DMLBADGE-RESNET-mkvzwgo0">View Credential</a></li> </ul> <hr/> <h2 id="why-these-matter">Why These Matter</h2> <p>Both <strong>Transformers</strong> and <strong>ResNet</strong> are foundational architectures in modern deep learning:</p> <ul> <li><strong>Transformers</strong> power state-of-the-art models in NLP (BERT, GPT), computer vision (ViT), and even speech recognition (like the ASR models I work on at Samsung).</li> <li><strong>ResNet</strong> introduced residual connections that solved the vanishing gradient problem, enabling networks with hundreds of layers to be trained effectively.</li> </ul> <p>These hands-on problem collections helped reinforce my understanding of the underlying mechanicsâ€”from self-attention mechanisms to skip connections.</p> <h2 id="whats-next">Whatâ€™s Next?</h2> <p>Iâ€™m looking forward to tackling more Deep-ML collections and continuing to deepen my practical understanding of deep learning architectures. Stay tuned for more updates!</p>]]></content><author><name></name></author><category term="achievements"/><category term="deep-learning"/><category term="certifications"/><category term="machine-learning"/><summary type="html"><![CDATA[Celebrating the completion of Deep-ML problem collections on Transformers and ResNet architectures.]]></summary></entry><entry><title type="html">Hello World</title><link href="https://yashjayswal24.github.io/portfolio/blog/2026/hello-world/" rel="alternate" type="text/html" title="Hello World"/><published>2026-01-23T21:51:00+00:00</published><updated>2026-01-23T21:51:00+00:00</updated><id>https://yashjayswal24.github.io/portfolio/blog/2026/hello-world</id><content type="html" xml:base="https://yashjayswal24.github.io/portfolio/blog/2026/hello-world/"><![CDATA[<p>Welcome to my new portfolio website!</p> <p>This is the first post on my new blog. Iâ€™ll be sharing updates about my projects, research, and other interests here.</p> <p>Stay tuned for more!</p>]]></content><author><name></name></author><category term="general"/><category term="introduction"/><summary type="html"><![CDATA[Welcome to my new portfolio website!]]></summary></entry></feed>